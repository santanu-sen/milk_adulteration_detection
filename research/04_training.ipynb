{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\admin\\\\Desktop\\\\JI\\\\Project\\\\capstone\\\\milk_adulteration_detection\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\admin\\\\Desktop\\\\JI\\\\Project\\\\capstone\\\\milk_adulteration_detection'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TrainingConfig:\n",
    "    root_dir: Path\n",
    "    trained_model_path: Path\n",
    "    updated_base_model_path: Path\n",
    "    training_data: Path\n",
    "    params_threshold: float\n",
    "    \n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class PrepareCallbacksConfig:\n",
    "    root_dir: Path\n",
    "    tensorboard_root_log_dir: Path\n",
    "    checkpoint_model_filepath: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detection.constants import *\n",
    "from detection.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self, \n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    \n",
    "    def get_prepare_callback_config(self) -> PrepareCallbacksConfig:\n",
    "        config = self.config.prepare_callbacks\n",
    "        model_ckpt_dir = os.path.dirname(config.checkpoint_model_filepath)\n",
    "        create_directories([\n",
    "            Path(model_ckpt_dir),\n",
    "            Path(config.tensorboard_root_log_dir)\n",
    "        ])\n",
    "\n",
    "        prepare_callback_config = PrepareCallbacksConfig(\n",
    "            root_dir=Path(config.root_dir),\n",
    "            tensorboard_root_log_dir=Path(config.tensorboard_root_log_dir),\n",
    "            checkpoint_model_filepath=Path(config.checkpoint_model_filepath)\n",
    "        )\n",
    "\n",
    "        return prepare_callback_config\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def get_training_config(self) -> TrainingConfig:\n",
    "        training = self.config.training\n",
    "        prepare_base_model = self.config.prepare_base_model\n",
    "        params = self.params\n",
    "        training_data = os.path.join(self.config.data_ingestion.root_dir, \"glucose\")\n",
    "        create_directories([\n",
    "            Path(training.root_dir)\n",
    "        ])\n",
    "\n",
    "        training_config = TrainingConfig(\n",
    "            root_dir=Path(training.root_dir),\n",
    "            trained_model_path=Path(training.trained_model_path),\n",
    "            updated_base_model_path=Path(prepare_base_model.updated_base_model_path),\n",
    "            training_data=Path(training_data),\n",
    "            params_threshold=params.THRESHOLD\n",
    "            \n",
    "        )\n",
    "\n",
    "        return training_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tensorboardX import SummaryWriter\n",
    "import shutil\n",
    "import joblib\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepareCallback:\n",
    "    def __init__(self, config: PrepareCallbacksConfig):\n",
    "        self.config = config\n",
    "\n",
    "    @property\n",
    "    def _create_tb_callbacks(self):\n",
    "        timestamp = time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "        tb_running_log_dir = os.path.join(self.config.tensorboard_root_log_dir, f\"tb_logs_at_{timestamp}\")\n",
    "        \n",
    "        # Remove the directory if it already exists\n",
    "        if os.path.exists(tb_running_log_dir):\n",
    "            shutil.rmtree(tb_running_log_dir)\n",
    "\n",
    "        # Create a SummaryWriter for TensorBoard logging\n",
    "        self.writer = SummaryWriter(log_dir=tb_running_log_dir)\n",
    "\n",
    "        def tb_callback(env):\n",
    "            for i in range(len(env.models)):\n",
    "                self.writer.add_scalar(f'error_{i}', env.evaluation_result_list[i][1], env.iteration)\n",
    "                self.writer.add_scalar(f'logloss_{i}', env.evaluation_result_list[i][2], env.iteration)\n",
    "\n",
    "        return tb_callback\n",
    "\n",
    "    @property\n",
    "    def _create_ckpt_callbacks(self):\n",
    "        checkpoint_path = self.config.checkpoint_model_filepath\n",
    "        \n",
    "        def ckpt_callback(env):\n",
    "            joblib.dump(env.model, checkpoint_path)\n",
    "\n",
    "        return ckpt_callback\n",
    "\n",
    "    def get_tb_ckpt_callbacks(self):\n",
    "        return [\n",
    "            self._create_tb_callbacks,\n",
    "            self._create_ckpt_callbacks\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request as request\n",
    "import xgboost as xgb\n",
    "import time\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Training:\n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        self.config = config\n",
    "    \n",
    "    def get_base_model(self):\n",
    "        self.model = joblib.load(\n",
    "            self.config.updated_base_model_path\n",
    "        )\n",
    "    \n",
    "    #function to load all images as data\n",
    "\n",
    "    def load_data(self, file_path, threshold  ):\n",
    "\n",
    "        file_path = self.config.training_data\n",
    "        threshold = self.config.params_threshold\n",
    "\n",
    "        folders = ['0', '0.1', '0.2', '0.3', '0.4', '0.5', '0.6', '0.7', '0.8', '0.9', '1', '1.5', '2', '2.5','3', '4', '5', '6', '7', '8']\n",
    "\n",
    "        #Define dataframe in which the image is saved\n",
    "\n",
    "        # Define column names\n",
    "        columns = ['Red Channel', 'Green Channel', 'Blue Channel', 'Hue', 'Saturation', 'Value', 'Lightness', 'channel a',  'channel b', 'Target']\n",
    "\n",
    "        # Create an empty DataFrame with columns\n",
    "        df = pd.DataFrame(columns=columns)\n",
    "\n",
    "        # Iterate through folders\n",
    "        for folder in folders:\n",
    "            folder_path = str(file_path) + folder\n",
    "\n",
    "            # Iterate through files in the current folder\n",
    "            for filename in os.listdir(folder_path):\n",
    "                # Check if the file has an image extension\n",
    "                if filename.lower().endswith(('.jpg', '.png', '.jpeg')):\n",
    "                    # Construct the full path to the image\n",
    "                    image_path = os.path.join(folder_path, filename)\n",
    "\n",
    "                    # Read the image (using OpenCV)\n",
    "                    img = cv2.imread(image_path)\n",
    "\n",
    "                    # Split the image into channels\n",
    "                    r, g, b = cv2.split(img)\n",
    "                    # Calculate the average of each channel\n",
    "                    average_r = r.mean()\n",
    "                    average_g = g.mean()\n",
    "                    average_b = b.mean()\n",
    "\n",
    "\n",
    "                    # Convert RGB image to HSV\n",
    "                    hsv_img = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
    "\n",
    "                    # Split the HSV image into channels\n",
    "                    h, s, v = cv2.split(hsv_img)\n",
    "                    # Calculate the average of each channel\n",
    "                    average_h = h.mean()\n",
    "                    average_s = s.mean()\n",
    "                    average_v = v.mean()\n",
    "\n",
    "                    # Convert RGB image to LAB\n",
    "                    img_lab = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n",
    "\n",
    "                    # Split LAB image into components\n",
    "                    L, a, b = cv2.split(img_lab)\n",
    "                    # Calculate the average of each channel\n",
    "                    average_L = L.mean()\n",
    "                    average_a = a.mean()\n",
    "                    average_b = b.mean()\n",
    "\n",
    "                    if float(folder) < threshold:\n",
    "                        # Append rows one by one\n",
    "                        new_row = {'Red Channel': average_r, 'Green Channel': average_g, 'Blue Channel': average_b, 'Hue': average_h, 'Saturation': average_s, 'Value': average_v, 'Lightness': average_L, 'channel a': average_a,  'channel b': average_b, 'Target': 0}\n",
    "                        df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "                    else:\n",
    "                        new_row = {'Red Channel': average_r, 'Green Channel': average_g, 'Blue Channel': average_b, 'Hue': average_h, 'Saturation': average_s, 'Value': average_v, 'Lightness': average_L, 'channel a': average_a,  'channel b': average_b, 'Target': 1}\n",
    "                        df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "        df['Target'] = df['Target'].astype(int)\n",
    "        return df\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def save_model(self, path: Path, model: xgb):\n",
    "        joblib.dump( model, path)\n",
    "\n",
    "    def train(self, df ):\n",
    "\n",
    "        df_shuffled = df.sample(frac=1, random_state=42)\n",
    "\n",
    "        # 'Target' column is the target variable\n",
    "        y = df_shuffled['Target']  # Target variable\n",
    "        X = df_shuffled.drop(columns=['Target'])  # Features\n",
    "\n",
    "        # Split the shuffled data into training and testing sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        self.model.fit(X_train, y_train)\n",
    "        # Predict on the test data\n",
    "        y_pred = self.model.predict(X_test)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        print('Accuracy is ', accuracy)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        self.save_model( self,\n",
    "            path=self.config.trained_model_path,\n",
    "            model=self.model\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-06-09 02:42:35,469: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-06-09 02:42:35,485: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-06-09 02:42:35,502: INFO: common: created directory at: artifacts]\n",
      "[2024-06-09 02:42:35,507: INFO: common: created directory at: artifacts\\prepare_callbacks\\checkpoint_dir]\n",
      "[2024-06-09 02:42:35,511: INFO: common: created directory at: artifacts\\prepare_callbacks\\xgbboard_log_dir]\n",
      "[2024-06-09 02:42:35,514: INFO: common: created directory at: artifacts\\training]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-06-09 02:42:35,517: INFO: common: created directory at: artifacts\\training]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_8640\\288825235.py:72: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is  0.8797814207650273\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    prepare_callbacks_config = config.get_prepare_callback_config()\n",
    "    prepare_callbacks = PrepareCallback(config=prepare_callbacks_config)\n",
    "    trainingConfig = config.get_training_config()\n",
    "\n",
    "    \n",
    "    file_path = trainingConfig.training_data\n",
    "    threshold = trainingConfig.params_threshold\n",
    "\n",
    "    training_config = config.get_training_config()\n",
    "    training = Training(config=training_config)\n",
    "    training.get_base_model()\n",
    "    df = training.load_data(file_path , threshold)\n",
    "    training.train(df)\n",
    "    \n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "milk_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
